{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Manually set the __file__ variable to the notebook's directory\n",
    "__file__ = os.path.abspath(\"notebook_name.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CFOLytics_reportgenerator_no_manual_fields.py\n",
    "\n",
    "LangGraph workflow that:\n",
    "1) Takes an initial user prompt in the conversation (state[\"messages\"]).\n",
    "2) Checks clarity using LLM instructions from 'verify_instructions.xml'.\n",
    "3) If unclear, LLM asks clarifying question -> user responds in the conversation -> we parse user’s new message -> re-check clarity.\n",
    "4) If clear, LLM generates a layout (render_layout.xml).\n",
    "5) LLM then asks user “Is this layout okay?” -> user answers in conversation -> we parse yes/no from the conversation -> if no, regenerate, if yes, proceed.\n",
    "6) Generate components, unify lists, finalize JSON.\n",
    "\n",
    "No need for manually adding `clarification_answer` or `layout_confirm` to the state. The conversation itself is the source of truth.\n",
    "\n",
    "Requires:\n",
    "- langgraph\n",
    "- langchain-core\n",
    "- langchain-community\n",
    "- langchain-openai\n",
    "- Your custom ChatGroq, or whichever LLM wrapper you use\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# LangChain / LangGraph\n",
    "from langchain_core.messages import (\n",
    "    AIMessage, \n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    BaseMessage\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.constants import START, END, Send\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "\n",
    "\n",
    "# Example: your custom ChatGroq usage\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    api_key=\"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\"\n",
    ")\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\",\n",
    "#         temperature=0)\n",
    "\n",
    "def load_xml_instructions(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Load system instructions from 'XML_instructions/filename' if you keep them externally.\n",
    "    Otherwise, just inline your prompts as strings.\n",
    "    \"\"\"\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    file_path = os.path.join(current_dir, \"XML_instructions\", filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "class FinalReportState(TypedDict):\n",
    "    \"\"\"\n",
    "    Our final JSON structures and clarity status.\n",
    "    \"\"\"\n",
    "    instructions_clear: bool\n",
    "    layout_json: dict\n",
    "    final_json: dict\n",
    "\n",
    "class ReportGraphState(MessagesState, FinalReportState):\n",
    "    \"\"\"\n",
    "    Merges the base conversation messages plus our custom fields.\n",
    "    'messages' is a list of SystemMessage, HumanMessage, or AIMessage.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) verify_instructions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def verify_instructions(state: ReportGraphState):\n",
    "    \"\"\"\n",
    "    Node checks if the conversation so far implies the instructions are clear or not.\n",
    "    We load instructions from 'verify_instructions.xml'.\n",
    "    - LLM appends a final line \"clear\" or \"not clear\" or \"unclear\" which we parse.\n",
    "    - We store the LLM output in the conversation.\n",
    "    \"\"\"\n",
    "    system_instructions = load_xml_instructions(\"verify_instructions.xml\")\n",
    "    system_msg = SystemMessage(content=system_instructions)\n",
    "\n",
    "    # We pass the entire conversation plus the system instructions.\n",
    "    conversation = [system_msg] + state[\"messages\"]\n",
    "    result = llm.invoke(conversation)\n",
    "\n",
    "    # Store the LLM's analysis as an AIMessage\n",
    "    state[\"messages\"].append(AIMessage(content=result.content, name=\"clarity-check\"))\n",
    "    \n",
    "    text_lower = result.content.lower()\n",
    "    if \"not clear\" in text_lower or \"unclear\" in text_lower:\n",
    "        return {\"instructions_clear\": False}\n",
    "    return {\"instructions_clear\": True}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) ask_clarification\n",
    "# -----------------------------------------------------------------------------\n",
    "def ask_clarification(state: ReportGraphState):\n",
    "    \"\"\"\n",
    "    LLM asks the user clarifying questions. We store that question in the conversation.\n",
    "    \"\"\"\n",
    "    system_instructions = load_xml_instructions(\"clarification_prompt.xml\")\n",
    "    system_msg = SystemMessage(content=system_instructions)\n",
    "\n",
    "    conversation = [system_msg] + state[\"messages\"]\n",
    "    llmvers = ChatGroq(temperature=0, model_name=\"llama-3.3-70b-versatile\", api_key=\"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\")\n",
    "    result = llmvers.invoke(conversation)\n",
    "\n",
    "    # Append the AI’s clarifying question\n",
    "    question_msg = AIMessage(content=result.content, name=\"clarification_question\")\n",
    "    state[\"messages\"].append(question_msg)\n",
    "\n",
    "    return {}  # No direct state changes, just updated conversation\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) get_user_clarification\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_user_clarification(state: ReportGraphState):\n",
    "    idx_question = None\n",
    "    for i, msg in reversed(list(enumerate(state[\"messages\"]))):\n",
    "        if isinstance(msg, AIMessage) and msg.name == \"clarification_question\":\n",
    "            idx_question = i\n",
    "            break\n",
    "    if idx_question is None:\n",
    "        return {}\n",
    "\n",
    "    for j in range(idx_question+1, len(state[\"messages\"])):\n",
    "        msg = state[\"messages\"][j]\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            # User responded, proceed to next node\n",
    "            return {}\n",
    "\n",
    "    # Remain in this node until the user responds\n",
    "    return None  # Signal to remain in the current node\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) generate_layout_json\n",
    "# -----------------------------------------------------------------------------\n",
    "def generate_layout_json(state: ReportGraphState):\n",
    "    \"\"\"\n",
    "    Generate a JSON layout using structured output (json_mode).\n",
    "    \"\"\"\n",
    "    # Load system instructions for generating the layout\n",
    "    system_instructions = load_xml_instructions(\"render_layout.xml\")\n",
    "    system_msg = SystemMessage(content=system_instructions)\n",
    "\n",
    "    # Combine system message with conversation history\n",
    "    conversation = [system_msg] + state[\"messages\"]\n",
    "\n",
    "    from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "    # Define the schema for the LLM's output\n",
    "    class LayoutConfig(BaseModel):\n",
    "        gridColumns: dict\n",
    "        rows: list\n",
    "\n",
    "    class ReportConfig(BaseModel):\n",
    "        reportTitle: str = Field(alias=\"reportTitle\")\n",
    "        layout: LayoutConfig\n",
    "        numberFormat: dict = Field(alias=\"numberFormat\")\n",
    "\n",
    "        class Config:\n",
    "            allow_population_by_field_name = True\n",
    "\n",
    "    # Initialize the structured LLM for parsing\n",
    "    structured_llm = llm.with_structured_output(\n",
    "        ReportConfig,\n",
    "        method=\"json_mode\",\n",
    "        include_raw=True\n",
    "    )\n",
    "\n",
    "    # Invoke the LLM and capture the structured output\n",
    "    output = structured_llm.invoke(conversation)\n",
    "\n",
    "    # Extract the parsed output\n",
    "    parsed_output = output.get(\"parsed\", None)\n",
    "\n",
    "    if parsed_output:\n",
    "        # Store the parsed layout in the state\n",
    "        state[\"layout_json\"] = parsed_output.dict(by_alias=True)\n",
    "    else:\n",
    "        # Handle parsing failure\n",
    "        state[\"layout_json\"] = {\n",
    "            \"error\": \"Failed to parse layout\",\n",
    "            \"raw_output\": output.raw if \"raw\" in output else None\n",
    "        }\n",
    "\n",
    "    # Return the updated state\n",
    "    return {\"layout_json\": state[\"layout_json\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 8) identify_and_unify_lists\n",
    "# -----------------------------------------------------------------------------\n",
    "def identify_and_unify_lists(state: ReportGraphState):\n",
    "    \"\"\"\n",
    "    Placeholder for list unification. We'll just pass layout through.\n",
    "    \"\"\"\n",
    "    layout_json = state.get(\"layout_json\", {})\n",
    "    return {\"layout_json\": layout_json}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9) create_lists_contents\n",
    "# -----------------------------------------------------------------------------\n",
    "def create_lists_contents(state: ReportGraphState):\n",
    "    \"\"\"\n",
    "    Finds 'lists' keys and populates them with dimension members. Hard-coded example.\n",
    "    \"\"\"\n",
    "    layout_json = state.get(\"layout_json\", {})\n",
    "    if not layout_json:\n",
    "        return {}\n",
    "\n",
    "    lists_found = []\n",
    "    def walk(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            if \"lists\" in obj and isinstance(obj[\"lists\"], list):\n",
    "                for l_ in obj[\"lists\"]:\n",
    "                    lists_found.append(l_)\n",
    "            for v in obj.values():\n",
    "                walk(v)\n",
    "        elif isinstance(obj, list):\n",
    "            for v in obj:\n",
    "                walk(v)\n",
    "    walk(layout_json)\n",
    "\n",
    "    for item in lists_found:\n",
    "        item[\"list\"] = [\"Jan\", \"Feb\", \"Mar\"]\n",
    "        if \"AI Generation Description\" not in item:\n",
    "            item[\"AI Generation Description\"] = \"Populated with months for demonstration.\"\n",
    "\n",
    "    return {\"layout_json\": layout_json}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 10) finalize_report_json\n",
    "# -----------------------------------------------------------------------------\n",
    "def finalize_report_json(state: ReportGraphState):\n",
    "    \"\"\"\n",
    "    Copy layout_json => final_json\n",
    "    \"\"\"\n",
    "    layout_json = state.get(\"layout_json\", {})\n",
    "    return {\"final_json\": layout_json}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_component(state: dict):\n",
    "    \"\"\"\n",
    "    Process a single component using the LLM with structured output and explicitly update the state.\n",
    "    \"\"\"\n",
    "    from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "    from typing import Optional\n",
    "\n",
    "    # Define a flexible schema for the component configuration\n",
    "    class ComponentConfig(BaseModel):\n",
    "        config: Optional[dict] = None\n",
    "\n",
    "    # Load system instructions\n",
    "    comp_instructions = load_xml_instructions(\"component_content_gen.xml\")\n",
    "    system_msg = SystemMessage(content=comp_instructions)\n",
    "\n",
    "    # Extract component description\n",
    "    desc = state.get(\"AI Generation Description\", \"No AI Generation Description was provided.\")\n",
    "\n",
    "    # Prepare the conversation\n",
    "    conversation = [system_msg] + [HumanMessage(content=desc, name=\"component-desc\")]\n",
    "\n",
    "    # Use the LLM with structured output\n",
    "    structured_llm = llm.with_structured_output(\n",
    "        ComponentConfig,\n",
    "        method=\"json_mode\",\n",
    "        include_raw=True\n",
    "    )\n",
    "    output = structured_llm.invoke(conversation)\n",
    "\n",
    "    # Extract parsed output or handle errors\n",
    "    parsed_output = output.get(\"parsed\", None)\n",
    "    if parsed_output:\n",
    "        config = parsed_output.dict(by_alias=True)\n",
    "    else:\n",
    "        config = {\n",
    "            \"error\": \"Failed to parse component configuration\",\n",
    "            \"raw_output\": output.raw if \"raw\" in output else None\n",
    "        }\n",
    "\n",
    "    # Return the result as an explicit state update\n",
    "    return {\"layout_json\": config}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_components_config(state: ReportGraphState):\n",
    "    \"\"\"\n",
    "    1) Gathers all components from layout_json.\n",
    "    2) Returns a list of Send(...) tasks for each component.\n",
    "       The main graph engine will handle invoking \"process_component\"\n",
    "       in parallel for each item and pass the combined results\n",
    "       in state[\"tasks\"] to the next node.\n",
    "    \"\"\"\n",
    "    layout_json = state.get(\"layout_json\", {})\n",
    "    if not layout_json or \"error\" in layout_json:\n",
    "        return {\"layout_json\": layout_json}  # no-op\n",
    "\n",
    "    # Collect all \"components\" from layout_json\n",
    "    components = []\n",
    "    def walk(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            if \"components\" in obj and isinstance(obj[\"components\"], list):\n",
    "                components.extend(obj[\"components\"])\n",
    "            for v in obj.values():\n",
    "                walk(v)\n",
    "        elif isinstance(obj, list):\n",
    "            for v in obj:\n",
    "                walk(v)\n",
    "\n",
    "    walk(layout_json)\n",
    "\n",
    "    # Return an array of parallel tasks:\n",
    "    # => The engine will run \"process_component\" once per component\n",
    "    return [Send(\"process_component\", component) for component in components]\n",
    "\n",
    "\n",
    "def merge_components_config(state: ReportGraphState):\n",
    "    \"\"\"\n",
    "    1) The LangGraph engine collects the results of all parallel\n",
    "       calls to 'process_component' in state[\"tasks\"].\n",
    "    2) We zip them with the original 'components' in layout_json\n",
    "       and inject the returned .config back into layout_json.\n",
    "    \"\"\"\n",
    "    layout_json = state.get(\"layout_json\", {})\n",
    "    tasks_results = state.get(\"tasks\", [])\n",
    "\n",
    "    if not tasks_results:\n",
    "        # means no parallel tasks or no components\n",
    "        return {\"layout_json\": layout_json}\n",
    "\n",
    "    # We must gather the same set of 'components' again to merge them properly\n",
    "    components = []\n",
    "    def walk(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            if \"components\" in obj and isinstance(obj[\"components\"], list):\n",
    "                components.extend(obj[\"components\"])\n",
    "            for v in obj.values():\n",
    "                walk(v)\n",
    "        elif isinstance(obj, list):\n",
    "            for v in obj:\n",
    "                walk(v)\n",
    "    walk(layout_json)\n",
    "\n",
    "    # Merge each result into the correct component\n",
    "    for component, result in zip(components, tasks_results):\n",
    "        new_config = result.get(\"config\", {})\n",
    "        walk_and_update(layout_json, component[\"id\"], new_config)\n",
    "\n",
    "    state[\"layout_json\"] = layout_json\n",
    "    return {\"layout_json\": layout_json}\n",
    "\n",
    "\n",
    "def walk_and_update(obj, comp_id, new_config):\n",
    "    \"\"\"\n",
    "    Helper that finds the dict whose \"id\" == comp_id and updates 'config'.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        if obj.get(\"id\") == comp_id:\n",
    "            obj[\"config\"] = new_config\n",
    "        for v in obj.values():\n",
    "            walk_and_update(v, comp_id, new_config)\n",
    "    elif isinstance(obj, list):\n",
    "        for v in obj:\n",
    "            walk_and_update(v, comp_id, new_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Build the Graph\n",
    "# -----------------------------------------------------------------------------\n",
    "builder = StateGraph(ReportGraphState)\n",
    "\n",
    "# 1. START => verify_instructions\n",
    "builder.add_node(\"verify_instructions\", verify_instructions)\n",
    "builder.add_edge(START, \"verify_instructions\")\n",
    "\n",
    "# 2. Decide: If instructions_clear => generate_layout_json, else => ask_clarification\n",
    "def instructions_decider(state: ReportGraphState):\n",
    "    return \"generate_layout_json\" if state[\"instructions_clear\"] else \"ask_clarification\"\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"verify_instructions\",\n",
    "    instructions_decider,\n",
    "    [\"generate_layout_json\", \"ask_clarification\"]\n",
    ")\n",
    "\n",
    "builder.add_node(\"ask_clarification\", ask_clarification)\n",
    "builder.add_node(\"get_user_clarification\", get_user_clarification)\n",
    "builder.add_node(\"generate_layout_json\", generate_layout_json)\n",
    "builder.add_node(\"merge_components_config\", merge_components_config)\n",
    "\n",
    "\n",
    "# Add process_component subgraph\n",
    "component_builder = StateGraph(dict)\n",
    "component_builder.add_node(\"process_component\", process_component)\n",
    "component_builder.add_edge(START, \"process_component\")\n",
    "component_builder.add_edge(\"process_component\", END)\n",
    "\n",
    "process_component_subgraph = component_builder.compile()\n",
    "\n",
    "# Add nodes for generate_components_config and other steps\n",
    "builder.add_node(\"generate_components_config\", generate_components_config)\n",
    "\n",
    "builder.add_node(\"identify_and_unify_lists\", identify_and_unify_lists)\n",
    "builder.add_node(\"create_lists_contents\", create_lists_contents)\n",
    "builder.add_node(\"finalize_report_json\", finalize_report_json)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(\"ask_clarification\", \"get_user_clarification\")\n",
    "builder.add_edge(\"get_user_clarification\", \"verify_instructions\")\n",
    "builder.add_edge(\"generate_layout_json\", \"generate_components_config\")\n",
    "builder.add_edge(\"generate_components_config\", \"merge_components_config\")\n",
    "builder.add_edge(\"merge_components_config\", \"identify_and_unify_lists\")\n",
    "\n",
    "# Continue with the rest of the flow\n",
    "builder.add_edge(\"identify_and_unify_lists\", \"create_lists_contents\")\n",
    "builder.add_edge(\"create_lists_contents\", \"finalize_report_json\")\n",
    "builder.add_edge(\"finalize_report_json\", END)\n",
    "\n",
    "# Finally, compile without using interrupt_before, because we rely on conversation-based logic\n",
    "graph = builder.compile(interrupt_before=[\"get_user_clarification\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = ReportGraphState(\n",
    "    messages=[\n",
    "        HumanMessage(content=\"Create a report showing the profit and loss in a table comparing actuals to budget. Next to the table I want to see a chart with 12 periods comparing Actuals to Budget for the current selected row in the table. Below the chart I want to see a small table breaking down the current selected line in to the product dimension.\", name=\"user\")\n",
    "    ],\n",
    "    instructions_clear=False,\n",
    "    layout_json={},\n",
    "    final_json={}\n",
    ")\n",
    "\n",
    "# Simulate clarification input by overriding `await_clarification_answer` in graph\n",
    "result_state = graph.invoke(state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "if \"layout_json\" in result_state:\n",
    "    display(Markdown(f\"**Layout JSON**:\\n```json\\n{json.dumps(result_state['layout_json'], indent=2)}\\n```\"))\n",
    "else:\n",
    "    display(Markdown(\"**No Layout JSON Generated**\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
